---
title: "Machine learning 2"
author: "Mara Acuja"
date: "24 11 2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# notes
```{r todos, include=TRUE}
print("The paper talks abotu 73 different countries/areas, but when visualizing them there are fewer dots on the maps --> investigate")
print("scatter plot für koordinaten und box plot für varianz in long und lat")
print("boxplot für distances für jedes modell -> um diese vergleichen zu können")
print("für finales ergebniss, ein karte, welche ist und soll koorinaten anzeigt und mit linien verbindet")
```

# librarys

```{r import_libs, include=TRUE}
#install.packages("installr")
#library("installr")
#install.Rtools()

#install.packages("ggmap")
#install.packages("maptools")
#install.packages("maps")
#install.packages("glmnet")
#install.packages("ISLR")
#TODO the one below necessary?
#install.packages("rgl")
#install.packages("dplyr")

library("glmnet")
library("ggplot2")
library("ggmap")
library("maptools")
library("maps")
library("dplyr")

library(ISLR)
options(rgl.printRglwidget = TRUE)
library(rgl)

```

# Comparison of machine learning algorithms / Introduction / Theory

This report attempts to compare two regression/classification algorithms on its behavior on a specific data set. What should be found out?


# Data set
Here should be a bit of a short summary of the data set with some key characteristics of the data set.

What is it about? Which variables are included? What type of variables?
What is missing? What about missing values?

```{r functions, include=TRUE}
distances <- function(predicted, actual_value) {
  dif <- predicted-actual_value
  dif <- dif * (40030/360) # scaling coordinates to km by the factor circumference (km) / 360°
  mse <- sqrt(dif[,1]^2 + dif[,2]^2)
  return(mse)
}
```

```{r import_files, include=TRUE}
data <- read.csv("Data/default_plus_chromatic_features_1059_tracks.txt", header=FALSE)
data <- as.data.frame(data)
colnames(data)[117:118] <- c("Latitude", "Longitude")
```

```{r preprocessing, include=TRUE}
# Maybe some more preprocessing could be done here.
anyNA(data) # testing if there is at least a single NA -> but in this dataset there isn't
# Maybe some more pre-processing could be done here.
anyDuplicated(data) # testing for duplicates -> 0 found

# feature scaling

# separate labels from features
#label_column_names <- c("Longitude", "Latitude")
#data_labels <- data[label_column_names]
#data_features <- subset(data, select = -label_column_names)
# scale features
#data_features_scaled <- as.data.frame(scale(data_features))
# add labels to the now scaled features
#data <- cbind(data_features_scaled, data_labels)


```

# some insights into the data

```{r check_if_standardized, include=TRUE, fig.align='center', out.width='.49\\linewidth', fig.show="hold"}
# check if data is already standardized

# get columns without the target cols ("long" and "lat")
data_without_target_cols <- subset(data, select=-c(Latitude,Longitude))

# for each column in the data get sd and median
sd_per_col <- apply(data_without_target_cols, 2, sd) # the two stands for columns, if we would have used 1 it would calculate the sd of the rows
sd_per_col_df <- data.frame(sd_per_col)

mean_per_col <- apply(data_without_target_cols, 2, mean)
mean_per_col_df <- data.frame(mean_per_col)

sd_and_mean_per_col_df <- merge(sd_per_col_df, mean_per_col_df, by="row.names", all=TRUE)

#par(mar = c(4, 4, .1, .1)) # to make the two plots show side by side and not above each other

boxplot(sd_per_col, data=sd_and_mean_per_col_df, xlab="standard deviation (blue line at 1)", y_lab="value", main="standard deviation of the 116 columns")
abline(h=1, col = "blue", lty=5)

boxplot(mean_per_col, data=sd_and_mean_per_col_df, xlab="mean (blue line at 0)", y_lab="value", main="mean of the 116 columns")
abline(h=0, col = "blue", lty=5)
```


```{r insights_create_worldmap, include=TRUE}
# basic world map with music origins
mapWorld <- borders("world", colour="gray50", fill="white")
mp <- ggplot() + mapWorld

mp + geom_point(data = data, aes(x = Longitude, y = Latitude), color = "red", alpha = 0.5)
```

# conspicuousness
When looking at the map, it seems like there are way fewer unique data points than expected. The paper states that there are 1,142 pieces from 73 countries/areas, but counting the point on the map just returnes 33 data points. The following code investigates this difference.

```{r insights_the_region_conspiracy, include=TRUE}
# investigate why there are fewer points on the map than regions on the map
# data is the full data set

# group data by unique combinations of long and lat and safe in data frame
# and count occurrences of each unique combination
# the unique combinations of long and lat represent regions
occurences_per_region <- data.frame(data %>% count(Longitude, Latitude, sort=TRUE))

nrow(occurences_per_region) # returns 33 -> 33 regions in the data set and not 71 like proposed in the paper -> maybe not all data has been uploaded 

sum(occurences_per_region[, 'n']) # returns 1059 -> therefore there are 1095 tracks in the data set
```

The results suggest that there are actually only 33 unique combinations of latitude and longitude in the data set. Therefore the pieces can only be categorized into 33 different categories. The reason for this discrepancy to the suggested number in the paper (73) might be that some regions have been aggregated prior to uploading the data or some row are missing. The second statement is also supported, by the fact that there are not 1,142 pieces in the data set as described in the paper, but only 1059 pieces. Whatever this (small) difference does not influence the tasked tackled/perused by this report.

```{r insights_draw_worldmap, include=TRUE}
plot(data$Longitude, data$Latitude, xlim=c(-180, 180), ylim=c(-90, 90))
```


```{r insights_3, include=TRUE}
df_lat <- data.frame(value = data$Latitude, dimension = "Latitude")
df_long <- data.frame(value = data$Longitude, dimension = "Longitude")
boxplot_df <- rbind(df_lat, df_long)

boxplot(value~dimension, data=boxplot_df)
```

```{r split_into_test_and_train, include=TRUE}
set.seed(1)
n <-dim(data)[1]
train <- sample(1:n, 0.8*n)
test <- (1:n)[-train]
```



# Method
Short summary about the algorithms. Which are used? What do we do? Classification or Regression?

## Baseline - Linear Regression

First, we take a baseline to get a basic understanding on how well our chosen algorithms perform. Therefore, we decided to use a linear regression. First we created a model which includes all variables. The lm() command cannot compute a model for both output variables at the same time. So it creates two separate linear models, one for each output variable:

```{r linear_regression_all_variables, include=TRUE}
model.lm.all <- lm(cbind(Longitude, Latitude)~., data=data[train,])
pred.all <- predict(model.lm.all, newdata=data[test,])
```

To calculate a good meaningful measurement for the goodness of fit for the predictions the distance from the true location is calculated. The distances are calculated as the euclidean distances of the Longitude and Latitude between the predictions and the true location. They are measured in [km]. These will be used for all the comparisons of the algorithms between each other but also with the baseline and also with the literature (we need here another citation to the original paper)

The predictions are on average quite far from the true destination:
```{r linear_regression_all_variables_result, include=TRUE, echo=FALSE}
mean(distances(pred.all, data[test, c("Latitude","Longitude")])) ## final result in km
```

The impact of the variables was analysed. It seemed that not all of them have a significant influence on the models. So a second model was developed, just using the variable which have a significiant influence on the full model, either on the Latitude or on the Longitude. 


```{r linear_regression_significant_variables, include=TRUE}
model.lm.sig <- lm(cbind(Longitude, Latitude)~ V4+V9+V16+V30+V32+V33+V37+V38+V61+V90+V91+V92+V95+V96
                   +V104+V5+V6+V8+V9+V11+V15+V34+V39+V63+V94+V97, 
                   data=data[train,])
pred.sig <- predict(model.lm.sig, newdata=data[test,])
```

The predictions for the smaller models are on average a bit better:
```{r linear_regression_significant_variables_result, include=TRUE, echo=FALSE}
mean(distances(pred.sig, data[test, c("Latitude","Longitude")])) ## final result in km
```

An ANOVA was calculated to check if there is a significant difference between the two models. 
```{r linear_regression_anova, include=TRUE}
anova(model.lm.all, model.lm.sig)
```

The model tells that there is a difference and, as seen before, the smaller model performs better.


```{r linear_regression_boxplot, echo=FALSE}
# TODO:  in GGPLOT2
boxplot(c(distances(pred.sig, data[test, c("Latitude","Longitude")]), distances(pred.all, data[test, c("Latitude","Longitude")]))) 
```

## Algorithm 1 - Ridge Regression

Short introduction of the first algorithm. What does it do? What are the strengths? What are weaknesses? How is it implemented, including major code snippets.

The first algorithm we tried was ridge regression. This algorithm is similar to a linear regression but while the linear regression tries to minimize the difference between the weighted input variables and the output data, the ridge regression adds a regularisation term on the input variables. 

[Here has to be added the formula of the ridge regression]

In fact, this is a possibility to fight over-fitting. If lambda is big the model tends to just take the b0 into account. So the predicted value is the mean of the output variable. If lambda is small, then the model tends to be normal non-regularised model, hence the one from the linear regression. 

The command glmnet is used to perform the ridge regression. Cross-validation is performed to find the optimal lambda values. In general, it would be possible with glmnet to just calculate one single model for both output variables. But unfortunately, this option is not available when doing the cross-validation. So, again two independent cross-validations are done to get two values for lambda, one for each output variable.

```{r ridge_data_preparation, include=TRUE, echo=FALSE}
x <- model.matrix(cbind(Longitude, Latitude)~., data)[,1:116]
y <- data[, c("Latitude","Longitude")]
```

```{r ridge_data_cv, include=TRUE, echo=FALSE}
ridge.mod.1 <- cv.glmnet(x[train,], y[train,1], alpha=0)
plot(ridge.mod.1) # TODO: Here has to be GGPLOT used
bestlam1 <- ridge.mod.1$lambda.min

ridge.mod.2 <- cv.glmnet(x[train,], y[train,2], alpha=0)
plot(ridge.mod.2) # TODO: Here has to be GGPLOT used
bestlam2 <- ridge.mod.2$lambda.min
```
It can be seen that the distributions for lambda are quite similar for both models. Although the MSE for both models differ the optimal lambda is quite similar:

```{r ridge_best_lambda, include=TRUE, echo=FALSE}
bestlam1
bestlam2
```

The model performs better with ridge regression as with the baseline. In this model two lambdas are used.

```{r ridge_data_two_lambdas, include=TRUE, echo=FALSE}
pred1 <- predict(ridge.mod.1, s=bestlam1, newx=x[test,])
pred2 <- predict(ridge.mod.2, s=bestlam2, newx=x[test,])

pred <- cbind(pred1, pred2)
mean(distances(pred, data[test, c("Latitude","Longitude")])) ## final result in km
```

A second result is calculated with just one lambda. This lambda is calculated as the mean of the before calculated values of lambda. The result for the adapted version is just slightly worse than with the model with two independent lambdas:

```{r ridge_data_mean_lambda, include=TRUE, echo=FALSE}
pred1 <- predict(ridge.mod.1, s=(bestlam1+bestlam2)/2, newx=x[test,])
pred2 <- predict(ridge.mod.2, s=(bestlam1+bestlam2)/2, newx=x[test,])


pred <- cbind(pred1, pred2)
mean(distances(pred, data[test, c("Latitude","Longitude")])) ## final result in km
```




```{r Matthias_playground_ridge, include=FALSE}
grid <- 10^seq(10, -2, length=100)
x <- model.matrix(cbind(Longitude, Latitude)~., data)[,1:116]
y <- data[, c("Latitude","Longitude")]
#set.seed(100)


## This is code for the evaluation how lambda for the models differ depending on the output variable (Latitude vs. Longitude)

#outputs <- matrix(nrow=100, ncol=2)
#for (r in 1:100) {
#  out1 <- cv.glmnet(x[train,], y[train,1], alpha=0)
#  #plot(out1)
#  bestlam <- out1$lambda.min
#  outputs[r,1] <- bestlam
#  out2 <- cv.glmnet(x[train,], y[train,2], alpha=0)
#  #plot(out2)
#  bestlam <- out2$lambda.min
#  outputs[r,2] <- bestlam
#}

#xx <- outputs[,1]
#yy <- outputs[,2]
#t.test(xx,yy)
#boxplot(outputs)

# Ridge regression with cross-validation depending on tw separate models
ridge.mod.1 <- cv.glmnet(x[train,], y[train,1], alpha=0)
plot(ridge.mod.1)
bestlam1 <- ridge.mod.1$lambda.min

ridge.mod.2 <- cv.glmnet(x[train,], y[train,2], alpha=0)
plot(ridge.mod.2)
bestlam2 <- ridge.mod.2$lambda.min

# predictions for two separate models
pred1 <- predict(ridge.mod.1, s=bestlam1, newx=x[test,])
pred2 <- predict(ridge.mod.2, s=bestlam2, newx=x[test,])

pred <- cbind(pred1, pred2)
mean(distances(pred, data[test, c("Latitude","Longitude")])) ## final result in km

# predictions for a model using just one lambda (mean)
pred1 <- predict(ridge.mod.1, s=(bestlam1+bestlam2)/2, newx=x[test,])
pred2 <- predict(ridge.mod.2, s=(bestlam1+bestlam2)/2, newx=x[test,])


pred <- cbind(pred1, pred2)
mean(distances(pred, data[test, c("Latitude","Longitude")])) ## final result in km

```


## Algorithm 2
Short introduction of the second algorithm. What does it do? What are the strengths? What are weaknesses? How is it implemented, including major code snippets.


# Results
Here some tables, summaries or especially graphs should be shown here. Maybe this section should be separated into two to show the algorithms for themselves


# Discussion
Here follows the discussion of the results. What are the major findings? How did the algorithms perform? Which one was better overall? Is it always better or were the findings which were better by the other one?
Which one should be implemented? How could the algorithm be tweaked to perform even better?
Where were the problems during implementation? Where are the limits for the algorithms?

How precise do we predict the cities? How far is the difference in kilometers? The authors of the paper where the dataset comes from have a mean great circle error of 3113km? Are we above or below and by how much?


# Conclusion
At final some conclusions about the key findings and which algorithm should be used. What was the goal? Were and how were they achieved?

# RMarkdown default stuff - needs to be removed but serves up to now as draft

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
