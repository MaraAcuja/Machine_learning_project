---
title: "Machine learning II"
subtitle: "Predicting the Geographical Origin of Music"
author: "Matthias Kunz, Stefan Beuchert, Adriana Sanchez, Vy Nguyen"
date: "11.01.2022"
#bibliography: bibliography.bib
output:
  pdf_document: 
    toc: true
    toc_depth: 2
  html_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# notes
```{r todos, include=TRUE}
print("scatter plot für koordinaten und box plot für varianz in long und lat") # why did we want this? its in the project, but the scatter plot is basically just the map without the cool parts 
print("boxplot für distances für jedes modell -> um diese vergleichen zu können")
print("für finales ergebniss, ein karte, welche ? ist und soll koorinaten anzeigt und mit linien verbindet")
```

```{r todos_from_matthias, include=TRUE}
# remove the normal two model ridge regression and replace it by just the combined model (Should we or not?) [Matthias]

# rename Vys code chunks --> finished

# have a look at the weights of the ridge regression and make a simplified model with less 
# variables

# test other model combinations for the NN to improve it. 

# add the algorithm to the ridge regression --> finsihed

# add a description to the distance metric [Matthias] --> finished

# add a small text to the standardizsation of the data [Stefan]

# change order of insights in the data and the main metrics --> finished 

# "conspicuousness" --> new name and maybe the graph after the image of the map

# remove locations without map below [Stefan] --> finished

# text for the boxplot of latitude and longitude [Stefan]

# where should be the data preparation? maybe more up at the insights [Matthias] --> finished

### baseline as combined model as well? [Matthias]

# force anova output just on one page [Matthias] --> too complicated

## include the graphs of the cv as images [Matthias]

# remove description text for sections (NN, Ridge Regression, Method, Data Set, Introduction) [Adriana]

# more text to explain the findings of the NN. What does the numbers mean? [Vy, Adriana]

# check the spelling of crossvalidation, cross validation, cross-validation, should be just one version --> finished


## results 
# compare boxplots of measurements there
# compare values completely


## text for conclusion


#replace normal plots in ggplot
#add + theme_bw() to each ggplot

```



```{r import_libs, include=FALSE, echo=FALSE}
#install.packages("installr")
#library("installr")
#install.Rtools()

#install.packages("ggmap")
#install.packages("maptools")
#install.packages("maps")
#install.packages("glmnet")
#install.packages("ISLR")
#TODO the one below necessary?
#install.packages("rgl")
#install.packages("dplyr")
#install.packages("keras")
#install.packages("tensorflow")

library("glmnet")
library("ggplot2")
library("ggmap")
library("maptools")
library("maps")
library("dplyr")

library(ISLR)
options(rgl.printRglwidget = TRUE)
library(rgl)

library(keras)
library(tensorflow)

```

# Comparison of machine learning algorithms / Introduction / Theory

This report attempts to compare two regression/classification algorithms on its behavior on a specific data set. What should be found out?

Ethnomusicology is the study of music in its social and cultural contexts. Being a highly interdisciplinary field of research, we found interesting trying to apply some machine learning algorithms: ridge regression and neural networks  and compare its results.  

# Data set
Here should be a bit of a short summary of the data set with some key characteristics of the data set.

What is it about? Which variables are included? What type of variables?
What is missing? What about missing values?

The data used in this project is provided by the authors of the paper "Predicting the Geographical Origin of Music"[@Zhou2015 how crated and used this set as basis for this paper. The key information embedded in this data set are attributes of music pieces labeled by there point of origin.

The authors collected 1,142 pieces of music from a personal CD-collection. The target values represented by longitude and latitude are the main country/area of residence of the artists that produced the music. Over all the authors collected music from 73 countries/areas not including western style music, since this category is called to have a global influence and therefor unfitted for predicting a specific country/region. 

The attributes to describe the music pieces are automatically created by a software called MARSYAS[@tzanetakis2007marsyas], a software created to extract audio features from wave files. With MARSYAS the authors could convert every music peace to a set of 116 features called 'chromatic features'. The features are numerical and the authors claim to have normalized them into a gaussian normal distribution.

The selected task is suitable for classification and regression. However we decided that regression is a better approach because in first instance we don't have enough data to be representative to each country and secondly because of the special characteristics of desired output, coordinates.

In order to deal with our spatial output data we created a function to scale from our predicted and real values for further model evaluation.


```{r import_files, include=TRUE, echo=FALSE}
data <- read.csv("Data/default_plus_chromatic_features_1059_tracks.txt", header=FALSE)
data <- as.data.frame(data)
colnames(data)[117:118] <- c("Latitude", "Longitude")
```

```{r preprocessing, include=TRUE}
# Maybe some more preprocessing could be done here.
anyNA(data) # testing if there is at least a single NA -> but in this dataset there isn't
# Maybe some more pre-processing could be done here.
anyDuplicated(data) # testing for duplicates -> 0 found

```

## some insights into the data

```{r check_if_standardized, include=TRUE, fig.align='center', out.width='.49\\linewidth', fig.show="hold", echo=FALSE}
# check if data is already standardized

# get columns without the target cols ("long" and "lat")
data_without_target_cols <- subset(data, select=-c(Latitude,Longitude))

# for each column in the data get sd and median
sd_per_col <- apply(data_without_target_cols, 2, sd) # the two stands for columns, if we would have used 1 it would calculate the sd of the rows
sd_per_col_df <- data.frame(sd_per_col)

mean_per_col <- apply(data_without_target_cols, 2, mean)
mean_per_col_df <- data.frame(mean_per_col)

sd_and_mean_per_col_df <- merge(sd_per_col_df, mean_per_col_df, by="row.names", all=TRUE)

#par(mar = c(4, 4, .1, .1)) # to make the two plots show side by side and not above each other

boxplot(sd_per_col, data=sd_and_mean_per_col_df, xlab="standard deviation (blue line at 1)", y_lab="value", main="standard deviation of the 116 columns")
abline(h=1, col = "blue", lty=5)

boxplot(mean_per_col, data=sd_and_mean_per_col_df, xlab="mean (blue line at 0)", y_lab="value", main="mean of the 116 columns")
abline(h=0, col = "blue", lty=5)
```


```{r insights_create_worldmap, include=TRUE, echo=FALSE}
# basic world map with music origins
mapWorld <- borders("world", colour="gray50", fill="white")
mp <- ggplot() + mapWorld

mp + geom_point(data = data, aes(x = Longitude, y = Latitude), color = "red", alpha = 0.5)
```

## Conspicuousness
When looking at the map, it seems like there are way fewer unique data points than expected. The paper states that there are 1,142 pieces from 73 countries/areas, but counting the point on the map just returns 33 data points. The following code investigates this difference.

```{r insights_the_region_conspiracy, include=TRUE}
# investigate why there are fewer points on the map than regions on the map
# data is the full data set

# group data by unique combinations of long and lat and safe in data frame
# and count occurrences of each unique combination
# the unique combinations of long and lat represent regions
occurences_per_region <- data.frame(data %>% count(Longitude, Latitude, sort=TRUE))

nrow(occurences_per_region) # returns 33 -> 33 regions in the data set and not 71 like proposed in the paper -> maybe not all data has been uploaded 

sum(occurences_per_region[, 'n']) # returns 1059 -> therefore there are 1095 tracks in the data set
```

The results suggest that there are actually only 33 unique combinations of latitude and longitude in the data set. Therefore the pieces can only be categorized into 33 different categories. The reason for this discrepancy to the suggested number in the paper (73) might be that some regions have been aggregated prior to uploading the data or some row are missing. The second statement is also supported, by the fact that there are not 1,142 pieces in the data set as described in the paper, but only 1059 pieces. Whatever this (small) difference does not influence the tasked tackled/perused by this report.


```{r insights_3, include=TRUE, echo=FALSE}
df_lat <- data.frame(value = data$Latitude, dimension = "Latitude")
df_long <- data.frame(value = data$Longitude, dimension = "Longitude")
boxplot_df <- rbind(df_lat, df_long)

boxplot(value~dimension, data=boxplot_df)
```

## Data preparation

The data is divided into a training set (80%) and into a test set (20%). A separate validation set was not defined as cross-validation was used or the validation was done directly within the algorithm.

```{r split_into_test_and_train, include=TRUE}
set.seed(1)
n <-dim(data)[1]
train <- sample(1:n, 0.8*n)
test <- (1:n)[-train]
```



# Method

We treat the problem of predicting the geographical origin of music as a regression problem since we want to predict the spherical coordinates (longitude and latitude).
In Zhou et al. [@Zhou2015] they list two reasons as to why it's preferably not treated as a classification problem: One, the ratio between the large number of countries/areas and the number of examples per country/area is very disproportional and would result in poor classification results. And secondly, with regression we already have a natural error metric: The geographical distance from the true position.

```{r functions, include=TRUE}
distances <- function(predicted, actual_value) {
  dif <- predicted-actual_value
  dif <- dif * (40030/360) # scaling coordinates to km by the factor circumference (km) / 360°
  mse <- sqrt(dif[,1]^2 + dif[,2]^2)
  return(mse)
}
```
We wrote an overall function that calculates the distance between the predicted location of origin and the actual origin. The euclidean distance is used as a metrics. In fact, the algorithm is quite close to the calculation of the mean squared error for two dimensions. But it also converts the sphere coordinates into distances in [km]. This allows an easy interpretation of the results as well as a comparison with the results from the literature.

To analyze the data we first took a baseline using a linear regression model before we applied two algorithms: Ridge regression and neural networks

## Baseline - Linear Regression

First, we take a baseline to get a basic understanding on how well our chosen algorithms perform. Therefore, we decided to use a linear regression. First we created a model which includes all variables. The lm() command cannot compute a model for both output variables at the same time. So it creates two separate linear models, one for each output variable:

```{r linear_regression_all_variables, include=TRUE}
model.lm.all <- lm(cbind(Longitude, Latitude)~., data=data[train,])
pred.all <- predict(model.lm.all, newdata=data[test,])
```

To calculate a good meaningful measurement for the goodness of fit for the predictions the distance from the true location is calculated. The distances are calculated as the euclidean distances of the Longitude and Latitude between the predictions and the true location. They are measured in [km]. These will be used for all the comparisons of the algorithms between each other but also with the baseline and also with the literature (we need here another citation to the original paper)

The predictions are on average quite far from the true destination:
```{r linear_regression_all_variables_result, include=TRUE, echo=FALSE}
mean(distances(pred.all, data[test, c("Latitude","Longitude")])) ## final result in km
```

The impact of the variables was analysed. It seemed that not all of them have a significant influence on the models. So a second model was developed, just using the variable which have a significant influence on the full model, either on the Latitude or on the Longitude. 


```{r linear_regression_significant_variables, include=TRUE}
model.lm.sig <- lm(cbind(Longitude, Latitude)~ V4+V9+V16+V30+V32+V33+V37+V38+V61+V90+V91
                   +V92+V95+V96+V104+V5+V6+V8+V9+V11+V15+V34+V39+V63+V94+V97, 
                   data=data[train,])
pred.sig <- predict(model.lm.sig, newdata=data[test,])
```

The predictions for the smaller models are on average a bit better:
```{r linear_regression_significant_variables_result, include=TRUE, echo=FALSE}
mean(distances(pred.sig, data[test, c("Latitude","Longitude")])) ## final result in km
```

An ANOVA was calculated to check if there is a significant difference between the two models. 
```{r linear_regression_anova, include=TRUE, echo=FALSE}
anova(model.lm.all, model.lm.sig)
```

The model tells that there is a difference and, as seen before, the smaller model performs better.


```{r linear_regression_boxplot, echo=FALSE}
# TODO:  in GGPLOT2
boxplot(c(distances(pred.sig, data[test, c("Latitude","Longitude")]), distances(pred.all, data[test, c("Latitude","Longitude")]))) 
```

## Algorithm 1 - Ridge Regression

Short introduction of the first algorithm. What does it do? What are the strengths? What are weaknesses? How is it implemented, including major code snippets.

The first algorithm we tried was ridge regression. This algorithm is similar to a linear regression but while the linear regression tries to minimize the difference between the weighted input variables and the output data, the ridge regression adds a regularisation term on the input variables. 

$$RSS + \lambda \sum_{j=1} ^{p} { \hat{ \beta _j}  } ^2 = \sum_{i=1} ^n (y_i - \hat { \beta_0 } - \sum_{j=1} ^p { \hat  {\beta_j}x_ij})^2 + \lambda \sum_{j=1} ^{p} { \hat{ \beta }  } ^2_j$$

In fact, this is a possibility to fight over-fitting. If lambda is big the model tends to just take the b0 into account. So the predicted value is the mean of the output variable. If lambda is small, then the model tends to be normal non-regularised model, hence the one from the linear regression. 

The command glmnet is used to perform the ridge regression. Cross-validation is performed to find the optimal lambda values. In general, it would be possible with glmnet to just calculate one single model for both output variables. But unfortunately, this option is not available when doing the cross-validation. So, again two independent cross-validations are done to get two values for lambda, one for each output variable.

```{r ridge_data_preparation, include=TRUE, echo=FALSE}
x <- model.matrix(cbind(Longitude, Latitude)~., data)[,1:116]
y <- data[, c("Latitude", "Longitude")]

print(class(y['Latitude']))
```

```{r ridge_data_cv, include=TRUE, echo=FALSE}
ridge.mod.1 <- cv.glmnet(x[train,], y[train,1], alpha=0)
plot(ridge.mod.1) # TODO: Here has to be GGPLOT used
bestlam1 <- ridge.mod.1$lambda.min

ridge.mod.2 <- cv.glmnet(x[train,], y[train,2], alpha=0)
plot(ridge.mod.2) # TODO: Here has to be GGPLOT used
bestlam2 <- ridge.mod.2$lambda.min
```
It can be seen that the distributions for lambda are quite similar for both models. Although the MSE for both models differ the optimal lambda is quite similar:

```{r ridge_best_lambda, include=TRUE, echo=FALSE}
bestlam1
bestlam2
```

The model performs better with ridge regression as with the baseline. In this model two lambdas are used.

```{r ridge_data_two_lambdas, include=TRUE, echo=FALSE}
pred1 <- predict(ridge.mod.1, s=bestlam1, newx=x[test,])
pred2 <- predict(ridge.mod.2, s=bestlam2, newx=x[test,])

pred <- cbind(pred1, pred2)
mean(distances(pred, data[test, c("Latitude","Longitude")])) ## final result in km
```

A second result is calculated with just one lambda. This lambda is calculated as the mean of the before calculated values of lambda. The result for the adapted version is just slightly worse than with the model with two independent lambdas:

```{r ridge_data_mean_lambda, include=TRUE, echo=FALSE}
pred1 <- predict(ridge.mod.1, s=(bestlam1+bestlam2)/2, newx=x[test,])
pred2 <- predict(ridge.mod.2, s=(bestlam1+bestlam2)/2, newx=x[test,])



pred_ridge_regression <- cbind(pred1, pred2)
pred_ridge_regression
mean(distances(pred_ridge_regression, data[test, c("Latitude","Longitude")])) ## final result in km
```

There is also a multivariate ridge regression model calculated, hence a single model with two outputs for latitude and longitude. The lambda is found by cross-validation. (This process was done in a separate file as it took more than an hour to get a good value for. But the processed code is shown below):

```{r ridge_cv_code, include=TRUE, echo=TRUE, eval=FALSE}
# cross validation function similiar to the CV lesson in ML 1
cross.validation <- function(grid) {
  cv.err <- rep(NA, length(grid))
  tic("Cross-Validation")
  for (k in 1:length(grid)) {
    
    cv.loo <- rep(NA, n)
    
    for (i in 1:n) {
      # defining data as LOOCV
      loo.x <- x.train[-i,]
      loo.y <- y.train[-i,]
      # do multivariate regression model
      loo.fit <- glmnet(loo.x, loo.y, family = "mgaussian", alpha=0, lambda=grid[k])
      pred.y <- predict(loo.fit, s=grid[k], newx=t(as.matrix(x.train[i,])))
      cv.loo[i] <- distances(pred.y, y.train[i,])
    }
    cv.err[k] <- mean(cv.loo)
  }
  toc()
  return (cv.err)
}
```

With the so found best lambda the multivariate model was developed. The best lambda is:

```{r best_lambda, include=TRUE, echo=FALSE}
bestlambda <- 0.007
```

```{r ridge_multivariate_model, include=TRUE, echo=FALSE}
ridge.mod.multivar <- glmnet(x[train,], y[train,], family = "mgaussian", alpha=0, lambda=bestlambda)
pred_ridge_regression <- predict(ridge.mod.multivar, s = bestlambda, newx=x[test,])
pred_ridge_regression <- pred_ridge_regression[,,1]
mean(distances(pred_ridge_regression, data[test, c("Latitude","Longitude")])) ## final result in km
```



## Algorithm 2 - Artificial Neural Networks (NNs)
Short introduction of the second algorithm. What does it do? What are the strengths? What are weaknesses? How is it implemented, including major code snippets.

For the second algorithm we'll be using Artificial Neural Networks (NNs) as they can be used for regression problems as well. 
To use NNs for our predictions we will, as with all machine learning models, fit the data to the model. This is done by minimizing the loss function, which for NN-regression is the mean of squared errors MSE over the training set.
In the case of fitting a neural network, it is much faster if the data are scaled/normalised first and also gives more appropriate results if the features that are used have different scales and ranges. But this is already the case here.

The NN takes all features from the training data and outputs two values for the Latitude and Longitude, so it is a multi-output regression. In-between Input and Output Layer are a few more layers defined which all use ReLU as activation function. For compiling and fitting the model we again use the Mean Squared Error (MSE) and Adam as the optimizer. We let the model train for 100 epochs with a batch size of 128 and validation split of 0.2.

The results are visualized below.

```{r neural_network, include=TRUE, echo=FALSE}
X_train<-as.matrix(x[train,])
y_train<-as.matrix(y[train,])
X_test<-as.matrix(x[test,])
y_test<-as.matrix(y[test,])

# Initialize a sequential model
model <- keras_model_sequential() 

# Add layers to the model
model %>% 
    layer_dense(units=64, activation = "relu", input_shape = c(116)) %>% 
    #layer_dense(units=32, activation = "relu") %>% 
    layer_dense(units=16, activation = "relu") %>% 
    layer_dense(units=8, activation = "relu") %>% 
    #layer_dense(units=4, activation = "relu") %>% 
    layer_dense(units=2, activation="linear")

# Compile the model
model %>% compile(
     loss = 'mse',
     optimizer = 'adam',
     metrics = list("mean_squared_error", "mean_absolute_error")
 )

model %>% summary()

# Fit the model and store the fitting history in `history` 
history <- model %>% fit(
     X_train, 
     y_train, 
     epochs = 100,
     batch_size = 128, 
     validation_split = 0.2
 )

# Plot the history
plot(history)

# Plot the MSE of the training data 
plot(history$metrics$mean_squared_error, main="Model's Mean Squared Error", xlab = "epoch", ylab="mae", col="blue", type="l")

# Plot the MSE of the validation data
lines(history$metrics$val_mean_squared_error, col="green")

# Add Legend
legend("topright", c("train","val_train"), col=c("blue", "green"), lty=c(1,1))

# MSE of train data
y_preds_train<-model %>% predict(X_train)
mean((y_train - y_preds_train)^2)

# Customized distance metric
mean(distances(y_train, y_preds_train)) 
```

For evaluation we predict the Longitude and Latitude for the same test data as for the other regression cases and use the MSE as well as the custom distance metric. We get the following results:

```{r NN_distance_metric, include=TRUE, echo=FALSE}
### Test data ###

# Predict the Latitude for the test data
y_pred_neural_net <- model %>% predict(X_test)

# Evaluate with MSE
mean((y_test - y_pred_neural_net)^2)

# Evaluate on test data and labels with customized distance metric
mean(distances(y_test, y_pred_neural_net))                   # 4122.794
```

# Results

```{r final_map_for_comparison, include=TRUE, echo=FALSE}
# y[test,]
target_values <- data.frame (Latitude = y[test,1],
                             Longitude = y[test,2])

target_values$index <- 1:nrow(target_values)
target_values$source <- "Original"

# y_pred_neural_net
neural_net_results <- data.frame (Latitude = y_pred_neural_net[,1],
                                  Longitude = y_pred_neural_net[,2])
neural_net_results$index <- 1:nrow(neural_net_results)
neural_net_results$source <- "NeuralNet"

# pred_ridge_regression
ridge_regression_results <- data.frame (Latitude = pred_ridge_regression[,1],
                                        Longitude = pred_ridge_regression[,2])
ridge_regression_results$index <- 1:nrow(ridge_regression_results)
ridge_regression_results$source <- "RidgeRegression"


dim(target_values)
dim(neural_net_results)
dim(ridge_regression_results)

aggregated_results_ridge_regression <- rbind(target_values, ridge_regression_results)
aggregated_results_neural_net <- rbind(target_values, neural_net_results)



nrow(aggregated_results_neural_net) # there are 1271 rows in each df

# because there are so many rows in each df we selct only 10% for the visualization
aggregated_results_ridge_regression_10_percent = aggregated_results_ridge_regression[aggregated_results_ridge_regression$index %% 10 == 0,]

aggregated_results_neural_net_10_percent = aggregated_results_neural_net[aggregated_results_neural_net$index %% 10 == 0,]

mapWorld <- borders("world", colour="gray50", fill="white")
mp <- ggplot() + mapWorld +
  geom_point(size = 1, data = aggregated_results_ridge_regression_10_percent, aes(Longitude, Latitude, group=index, color=source)) +
  geom_line(data = aggregated_results_ridge_regression_10_percent, aes(Longitude, Latitude, group=index, color="RidgeRegression")) +
  geom_point(size = 1, data = aggregated_results_neural_net_10_percent, aes(Longitude, Latitude, group=index, color=source)) +
  geom_line(data = aggregated_results_neural_net_10_percent, aes(Longitude, Latitude, group=index, color="NeuralNet"))
mp
```

Here some tables, summaries or especially graphs should be shown here. Maybe this section should be separated into two to show the algorithms for themselves

We decided on compare a traditional regression method against a more modern architecture like neural network assuming that the last one would perform better overall. First, in our implementation of ridge regression we are fitting two separate models in order to get the two desired output variables. Nevertheless, we know that latitude and longitude are correlated variables, fact that ridge regression doesn't take into account so we expected a bigger error than NN. In the contrary, above implementation of neural network is predicting two variables at once like examples given in paper. 

# Discussion
 
Here follows the discussion of the results. What are the major findings? How did the algorithms perform? Which one was better overall? Is it always better or were the findings which were better by the other one?
Which one should be implemented? How could the algorithm be tweaked to perform even better?
Where were the problems during implementation? Where are the limits for the algorithms?
How precise do we predict the cities? How far is the difference in kilometers? The authors of the paper where the dataset comes from have a mean great circle error of 3113km? Are we above or below and by how much?

As we introduced, predicting data points on the Earth using a latitude/longitude representation adds complexity to our problem because of the natural characteristics of coordinates. In first place longitude is discontinuous, meaning that the longitude of two points geographically near might be significantly different and secondly because coordinates are not linear. 

If we compare the two models applied by looking at MSE we know that neural network performs better than ridge regression, as we assumed.However the difference is not big ___ km. Nevertheless, the algorithms presented in In Zhou et al. [@Zhou2015] (KNN and RFR) performs significantly better than ours, achieving an average distance error from 3,100 km to 3,600 km.


# Conclusion
At final some conclusions about the key findings and which algorithm should be used. What was the goal? Were and how were they achieved?
